name: Enhanced Test Reporting

on:
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened]
  push:
    branches: [main, develop]

concurrency:
  group: test-reporting-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_VERSION: "20.18.1"
  PNPM_VERSION: "9.15.4"
  NODE_OPTIONS: "--max-old-space-size=4096"

permissions:
  contents: read
  pull-requests: write
  checks: write
  actions: read

jobs:
  test-flakiness-detection:
    name: 🔄 Test Flakiness Detection
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'pull_request'
    env:
      VITEST_SILENT: 'true'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup PNPM with Caching
        uses: ./.github/actions/setup-pnpm
        with:
          node-version: ${{ env.NODE_VERSION }}
          pnpm-version: ${{ env.PNPM_VERSION }}

      - name: Run Tests Multiple Times for Flakiness Detection
        id: flakiness-test
        run: |
          echo "## 🔄 Flakiness Detection Results" > flakiness-report.md
          echo "" >> flakiness-report.md

          # Run tests 3 times to detect flaky tests
          mkdir -p flakiness-results
          FLAKY_TESTS_FOUND=false
          ALL_RUNS_PASSED=true

          for run in {1..3}; do
            echo "### Run $run" >> flakiness-report.md
            echo "Running test iteration $run..."

            if pnpm test -- --reporter=json --outputFile=flakiness-results/test-run-$run.json 2>&1; then
              echo "✅ Run $run: PASSED" >> flakiness-report.md
            else
              echo "❌ Run $run: FAILED" >> flakiness-report.md
              ALL_RUNS_PASSED=false
            fi
          done

          echo "" >> flakiness-report.md

          # Analyze results for flakiness
          if [ "$ALL_RUNS_PASSED" = true ]; then
            echo "✅ **All test runs passed consistently**" >> flakiness-report.md
            echo "flaky_tests_detected=false" >> $GITHUB_OUTPUT
          else
            echo "⚠️ **Inconsistent test results detected - possible flaky tests**" >> flakiness-report.md
            echo "" >> flakiness-report.md
            echo "**Investigation needed:**" >> flakiness-report.md
            echo "- Check for race conditions in tests" >> flakiness-report.md
            echo "- Review async/await usage" >> flakiness-report.md
            echo "- Verify proper test cleanup" >> flakiness-report.md
            echo "- Consider adding test timeouts" >> flakiness-report.md
            echo "flaky_tests_detected=true" >> $GITHUB_OUTPUT
            FLAKY_TESTS_FOUND=true
          fi

          # Add to step summary
          cat flakiness-report.md >> $GITHUB_STEP_SUMMARY

      - name: Upload Flakiness Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: flakiness-results-${{ github.run_number }}
          path: |
            flakiness-results/
            flakiness-report.md
          retention-days: 30

      - name: Comment Flakiness Results on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let flakinessReport = '';
            try {
              flakinessReport = fs.readFileSync('flakiness-report.md', 'utf8');
            } catch (error) {
              flakinessReport = '❌ Failed to read flakiness report';
            }

            const body = `${flakinessReport}

            ---
            *This flakiness detection runs tests multiple times to identify inconsistent behavior.*`;

            // Find existing flakiness comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const existingComment = comments.find(comment =>
              comment.user.login === 'github-actions[bot]' &&
              comment.body.includes('🔄 Flakiness Detection Results')
            );

            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

  comprehensive-test-report:
    name: 📊 Comprehensive Test Report
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      VITEST_SILENT: 'true'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup PNPM with Caching
        uses: ./.github/actions/setup-pnpm
        with:
          node-version: ${{ env.NODE_VERSION }}
          pnpm-version: ${{ env.PNPM_VERSION }}

      - name: Run Comprehensive Test Suite
        id: test-run
        run: |
          mkdir -p test-reports

          # Run tests with multiple reporters
          if pnpm test -- --reporter=verbose --reporter=json --outputFile=test-reports/test-results.json --reporter=junit --outputFile=test-reports/junit.xml; then
            echo "test_passed=true" >> $GITHUB_OUTPUT
          else
            echo "test_passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Parse Test Results
        id: parse-results
        run: |
          echo "## 📊 Test Results Summary" > test-summary.md
          echo "" >> test-summary.md

          if [ -f test-reports/test-results.json ]; then
            # Parse JSON results
            node -e "
              const fs = require('fs');
              const results = JSON.parse(fs.readFileSync('test-reports/test-results.json', 'utf8'));

              const stats = {
                total: 0,
                passed: 0,
                failed: 0,
                skipped: 0,
                duration: 0
              };

              if (results.testResults) {
                results.testResults.forEach(file => {
                  if (file.assertionResults) {
                    file.assertionResults.forEach(test => {
                      stats.total++;
                      if (test.status === 'passed') stats.passed++;
                      else if (test.status === 'failed') stats.failed++;
                      else if (test.status === 'pending' || test.status === 'skipped') stats.skipped++;
                    });
                  }
                });
                stats.duration = results.runTime || 0;
              }

              console.log('| Metric | Count | Percentage |');
              console.log('|--------|-------|------------|');
              console.log(\`| **Total Tests** | \${stats.total} | 100% |\`);
              console.log(\`| ✅ **Passed** | \${stats.passed} | \${stats.total > 0 ? ((stats.passed / stats.total) * 100).toFixed(1) : 0}% |\`);
              console.log(\`| ❌ **Failed** | \${stats.failed} | \${stats.total > 0 ? ((stats.failed / stats.total) * 100).toFixed(1) : 0}% |\`);
              console.log(\`| ⏭️ **Skipped** | \${stats.skipped} | \${stats.total > 0 ? ((stats.skipped / stats.total) * 100).toFixed(1) : 0}% |\`);
              console.log(\`| ⏱️ **Duration** | \${(stats.duration / 1000).toFixed(2)}s | - |\`);

              // Export for GitHub outputs
              console.error(\`total_tests=\${stats.total}\`);
              console.error(\`passed_tests=\${stats.passed}\`);
              console.error(\`failed_tests=\${stats.failed}\`);
              console.error(\`skipped_tests=\${stats.skipped}\`);
              console.error(\`test_duration=\${(stats.duration / 1000).toFixed(2)}\`);
            " >> test-summary.md 2>> github_outputs.txt

            # Set GitHub outputs
            if [ -f github_outputs.txt ]; then
              while IFS= read -r line; do
                echo "$line" >> $GITHUB_OUTPUT
              done < github_outputs.txt
            fi
          else
            echo "❌ Test results not found" >> test-summary.md
            echo "total_tests=0" >> $GITHUB_OUTPUT
            echo "passed_tests=0" >> $GITHUB_OUTPUT
            echo "failed_tests=0" >> $GITHUB_OUTPUT
            echo "skipped_tests=0" >> $GITHUB_OUTPUT
            echo "test_duration=0" >> $GITHUB_OUTPUT
          fi

          # Add to step summary
          cat test-summary.md >> $GITHUB_STEP_SUMMARY

      - name: Generate Test Performance Analysis
        run: |
          echo "" >> test-summary.md
          echo "## ⚡ Performance Analysis" >> test-summary.md
          echo "" >> test-summary.md

          DURATION=${{ steps.parse-results.outputs.test_duration }}
          TOTAL_TESTS=${{ steps.parse-results.outputs.total_tests }}

          if [ "$TOTAL_TESTS" -gt 0 ]; then
            AVG_TIME=$(echo "scale=3; $DURATION / $TOTAL_TESTS" | bc -l)
            echo "| Metric | Value |" >> test-summary.md
            echo "|--------|-------|" >> test-summary.md
            echo "| Total Duration | ${DURATION}s |" >> test-summary.md
            echo "| Average per Test | ${AVG_TIME}s |" >> test-summary.md
            echo "| Tests per Second | $(echo "scale=1; $TOTAL_TESTS / $DURATION" | bc -l) |" >> test-summary.md

            # Performance warnings
            if (( $(echo "$DURATION > 30" | bc -l) )); then
              echo "" >> test-summary.md
              echo "⚠️ **Performance Warning:** Test suite took longer than 30 seconds" >> test-summary.md
            fi

            if (( $(echo "$AVG_TIME > 1" | bc -l) )); then
              echo "" >> test-summary.md
              echo "⚠️ **Performance Warning:** Average test time is over 1 second" >> test-summary.md
            fi
          fi

      - name: Analyze Failed Tests
        if: steps.parse-results.outputs.failed_tests != '0'
        run: |
          echo "" >> test-summary.md
          echo "## ❌ Failed Test Analysis" >> test-summary.md
          echo "" >> test-summary.md

          if [ -f test-reports/test-results.json ]; then
            node -e "
              const results = JSON.parse(require('fs').readFileSync('test-reports/test-results.json', 'utf8'));
              let failedTests = [];

              if (results.testResults) {
                results.testResults.forEach(file => {
                  if (file.assertionResults) {
                    file.assertionResults.forEach(test => {
                      if (test.status === 'failed') {
                        failedTests.push({
                          name: test.title,
                          file: file.name,
                          error: test.failureMessages ? test.failureMessages[0] : 'Unknown error'
                        });
                      }
                    });
                  }
                });
              }

              if (failedTests.length > 0) {
                console.log('**Failed Tests:**');
                console.log('');
                failedTests.forEach((test, index) => {
                  console.log(\`\${index + 1}. **\${test.name}**\`);
                  console.log(\`   - File: \${test.file}\`);
                  console.log(\`   - Error: \\\`\${test.error.split('\\n')[0]}\\\`\`);
                  console.log('');
                });
              }
            " >> test-summary.md
          fi

      - name: Upload Test Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-reports-${{ github.run_number }}
          path: |
            test-reports/
            test-summary.md
          retention-days: 30

      - name: Comment Test Results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let testSummary = '';
            try {
              testSummary = fs.readFileSync('test-summary.md', 'utf8');
            } catch (error) {
              testSummary = '❌ Failed to read test summary';
            }

            const testPassed = ${{ steps.test-run.outputs.test_passed }};
            const totalTests = ${{ steps.parse-results.outputs.total_tests }};
            const passedTests = ${{ steps.parse-results.outputs.passed_tests }};
            const failedTests = ${{ steps.parse-results.outputs.failed_tests }};

            const body = `${testSummary}

            ## 🎯 Quick Summary
            ${testPassed ? '✅' : '❌'} **Overall Result:** ${testPassed ? 'PASSED' : 'FAILED'}
            📊 **Test Coverage:** ${totalTests > 0 ? ((passedTests / totalTests) * 100).toFixed(1) : 0}% (${passedTests}/${totalTests} tests passed)

            ${failedTests > 0 ? `
            🚨 **Action Required:** ${failedTests} test(s) failed. Please review and fix before merging.
            ` : ''}

            ---
            *Detailed test results are available in the \`comprehensive-test-reports-${context.runNumber}\` artifact.*`;

            // Find existing test comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const existingComment = comments.find(comment =>
              comment.user.login === 'github-actions[bot]' &&
              comment.body.includes('📊 Test Results Summary')
            );

            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Fail if Tests Failed
        if: steps.test-run.outputs.test_passed != 'true'
        run: |
          echo "❌ Tests failed. Please fix the failing tests before merging."
          exit 1